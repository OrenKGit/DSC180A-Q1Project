{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEHNNLayer(nn.Module):\n",
    "    def __init__(self, node_in_features, edge_in_features):\n",
    "        super(DEHNNLayer, self).__init__()\n",
    "        self.node_mlp1 = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp2 = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp3 = nn.Sequential(\n",
    "            nn.Linear(2 * node_in_features, 2 * node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.node_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_higher_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.higher_virtual_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_node_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Learnable defaults for missing driver or sink\n",
    "        self.default_driver = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_sink_agg = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_edge_agg = nn.Parameter(torch.zeros(edge_in_features))\n",
    "        self.default_virtual_node = nn.Parameter(torch.zeros(node_in_features))\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize all parameters with Xavier uniform distribution.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Node update\n",
    "        updated_node_features = {}\n",
    "        for node in hypergraph.nodes:\n",
    "            incident_edges = hypergraph.get_incident_edges(node)\n",
    "            if incident_edges:\n",
    "                agg_features = torch.sum(torch.stack([self.node_mlp1(edge_features[edge]) for edge in incident_edges]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_edge_agg\n",
    "            updated_node_features[node] = agg_features\n",
    "\n",
    "        # Edge update\n",
    "        updated_edge_features = {}\n",
    "        for edge in hypergraph.edges:\n",
    "            driver, sinks = hypergraph.get_driver_and_sinks(edge)\n",
    "\n",
    "            driver_feature = node_features[driver] if driver is not None else self.default_driver\n",
    "\n",
    "            if sinks:\n",
    "                sink_agg = torch.sum(torch.stack([self.edge_mlp2(node_features[sink]) for sink in sinks]), dim=0)\n",
    "            else:\n",
    "                sink_agg = self.default_sink_agg\n",
    "\n",
    "            concatenated = torch.cat([driver_feature, sink_agg])\n",
    "            updated_edge_features[edge] = self.edge_mlp3(concatenated)\n",
    "\n",
    "        # Virtual node aggregation\n",
    "        virtual_node_agg = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            assigned_nodes = [node for node in hypergraph.nodes if hypergraph.get_virtual_node(node) == virtual_node]\n",
    "            if assigned_nodes:\n",
    "                agg_features = torch.sum(torch.stack([self.node_to_virtual_mlp(node_features[node]) for node in assigned_nodes]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_virtual_node\n",
    "            virtual_node_agg[virtual_node] = agg_features\n",
    "\n",
    "        higher_virtual_feature = torch.sum(\n",
    "            torch.stack([self.virtual_to_higher_virtual_mlp(virtual_node_agg[vn]) for vn in virtual_node_agg]), dim=0\n",
    "        )\n",
    "\n",
    "        propagated_virtual_node_features = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            propagated_virtual_node_features[virtual_node] = self.higher_virtual_to_virtual_mlp(higher_virtual_feature)\n",
    "\n",
    "        for node in hypergraph.nodes:\n",
    "            virtual_node = hypergraph.get_virtual_node(node)\n",
    "            propagated_feature = self.virtual_to_node_mlp(propagated_virtual_node_features[virtual_node])\n",
    "            updated_node_features[node] += propagated_feature\n",
    "\n",
    "        return updated_node_features, updated_edge_features\n",
    "\n",
    "\n",
    "class DEHNN(nn.Module):\n",
    "    def __init__(self, num_layers, node_in_features, edge_in_features):\n",
    "        super(DEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DEHNNLayer(node_in_features, edge_in_features))\n",
    "            node_in_features, edge_in_features = edge_in_features, node_in_features\n",
    "            edge_in_features *= 2\n",
    "\n",
    "        edge_in_features = edge_in_features // 2\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(node_in_features, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        for layer in self.layers:\n",
    "            node_features, edge_features = layer(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        final_node_features = torch.stack([node_features[node] for node in hypergraph.nodes], dim=0)\n",
    "        output = self.output_layer(final_node_features)\n",
    "        return output\n",
    "    \n",
    "class Hypergraph:\n",
    "    def __init__(self, nodes, edges, driver_sink_map, node_to_virtual_map, num_virtual_nodes):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.driver_sink_map = driver_sink_map\n",
    "        self.node_to_virtual_map = node_to_virtual_map\n",
    "        self.num_virtual_nodes = num_virtual_nodes\n",
    "\n",
    "    def get_incident_edges(self, node):\n",
    "        return [edge for edge in self.edges if node in self.driver_sink_map[edge][1] or node == self.driver_sink_map[edge][0]]\n",
    "\n",
    "    def get_driver_and_sinks(self, edge):\n",
    "        return self.driver_sink_map[edge]\n",
    "    \n",
    "    def get_virtual_node(self, node):\n",
    "        return self.node_to_virtual_map[node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data and constructing hypergraph\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "clean_data_dir = '../data/chips/clean_data/'\n",
    "\n",
    "with open(clean_data_dir + '1.driver_sink_map.pkl', 'rb') as f:\n",
    "    train_driver_sink_map = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '1.node_features.pkl', 'rb') as f:\n",
    "    train_node_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '1.net_features.pkl', 'rb') as f:\n",
    "    train_edge_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '1.congestion.pkl', 'rb') as f:\n",
    "    train_congestion = pickle.load(f)\n",
    "\n",
    "train_partition = np.load(clean_data_dir + '1.partition.npy')\n",
    "\n",
    "train_node_features = {k: torch.tensor(v).float().to(device) for k, v in train_node_features.items()}\n",
    "train_edge_features = {k: torch.tensor(v).float().to(device) for k, v in train_edge_features.items()}\n",
    "\n",
    "train_nodes = list(range(len(train_node_features)))\n",
    "train_edges = list(range(len(train_edge_features)))\n",
    "train_hypergraph = Hypergraph(train_nodes, train_edges, train_driver_sink_map, train_partition, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading validation data and constructing hypergraph\n",
    "with open(clean_data_dir + '2.driver_sink_map.pkl', 'rb') as f:\n",
    "    val_driver_sink_map = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '2.node_features.pkl', 'rb') as f:\n",
    "    val_node_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '2.net_features.pkl', 'rb') as f:\n",
    "    val_edge_features = pickle.load(f)\n",
    "\n",
    "with open(clean_data_dir + '2.congestion.pkl', 'rb') as f:\n",
    "    val_congestion = pickle.load(f)\n",
    "\n",
    "val_partition = np.load(clean_data_dir + '2.partition.npy')\n",
    "\n",
    "val_node_features = {k: torch.tensor(v).float().to(device) for k, v in val_node_features.items()}\n",
    "val_edge_features = {k: torch.tensor(v).float().to(device) for k, v in val_edge_features.items()}\n",
    "\n",
    "val_nodes = list(range(len(val_node_features)))\n",
    "val_edges = list(range(len(val_edge_features)))\n",
    "val_hypergraph = Hypergraph(val_nodes, val_edges, val_driver_sink_map, val_partition, 2)\n",
    "val_targets = val_congestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037834691501746217\n",
      "0.0979251012145749\n"
     ]
    }
   ],
   "source": [
    "# checking class balance for validation and training\n",
    "print(np.array(list(val_targets.values())).mean())\n",
    "print(np.array(list(train_congestion.values())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train Loss: 5.1498\n",
      "Validation Loss: 0.3153, Validation Accuracy: 0.8545\n",
      "Epoch [2/10]\n",
      "Train Loss: 4.1136\n",
      "Validation Loss: 0.4343, Validation Accuracy: 0.7792\n",
      "Epoch [3/10]\n",
      "Train Loss: 3.1252\n",
      "Validation Loss: 0.6416, Validation Accuracy: 0.6708\n",
      "Epoch [4/10]\n",
      "Train Loss: 2.2031\n",
      "Validation Loss: 1.0277, Validation Accuracy: 0.4665\n",
      "Epoch [5/10]\n",
      "Train Loss: 1.3594\n",
      "Validation Loss: 3.2409, Validation Accuracy: 0.0038\n",
      "Epoch [6/10]\n",
      "Train Loss: 3.1144\n",
      "Validation Loss: 1.5024, Validation Accuracy: 0.1604\n",
      "Epoch [7/10]\n",
      "Train Loss: 0.8732\n",
      "Validation Loss: 0.8880, Validation Accuracy: 0.4229\n",
      "Epoch [8/10]\n",
      "Train Loss: 0.9605\n",
      "Validation Loss: 0.6206, Validation Accuracy: 0.5485\n",
      "Epoch [9/10]\n",
      "Train Loss: 1.0916\n",
      "Validation Loss: 0.4599, Validation Accuracy: 0.6916\n",
      "Epoch [10/10]\n",
      "Train Loss: 1.2093\n",
      "Validation Loss: 0.3601, Validation Accuracy: 0.8417\n"
     ]
    }
   ],
   "source": [
    "# Initialize DE-HNN model\n",
    "model = DEHNN(num_layers=2, node_in_features=14, edge_in_features=1).to(device)\n",
    "epochs = 10\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor (5.0))\n",
    "# Training and Validation Data\n",
    "train_node_features = {k: v.to(device) for k, v in train_node_features.items()}\n",
    "train_edge_features = {k: v.to(device) for k, v in train_edge_features.items()}\n",
    "train_targets = torch.tensor(list(train_congestion.values())).long().to(device)\n",
    "\n",
    "val_node_features = {k: v.to(device) for k, v in val_node_features.items()}\n",
    "val_edge_features = {k: v.to(device) for k, v in val_edge_features.items()}\n",
    "val_targets = torch.tensor(list(val_congestion.values())).long().to(device)\n",
    "\n",
    "# Training and Validation Loop \n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass for training\n",
    "    train_output = model(train_node_features, train_edge_features, train_hypergraph)\n",
    "    \n",
    "    # Compute training loss\n",
    "    train_loss = criterion(train_output, train_targets)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass for validation\n",
    "        val_output = model(val_node_features, val_edge_features, val_hypergraph)\n",
    "        \n",
    "        # Compute validation loss\n",
    "        val_loss = criterion(val_output, val_targets)\n",
    "        \n",
    "        # Compute validation accuracy\n",
    "        val_predictions = torch.argmax(val_output, dim=1)\n",
    "        val_correct = (val_predictions == val_targets).sum().item()\n",
    "        val_total = len(val_targets)\n",
    "        val_accuracy = val_correct / val_total\n",
    "    \n",
    "    # Print training and validation metrics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss.item():.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model(train_node_features, train_edge_features, train_hypergraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = test_output.detach().cpu().numpy()\n",
    "out = np.array([np.argmax(i) for i in out])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8598178137651822"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "np.mean(np.array(list(train_congestion.values())) == out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
